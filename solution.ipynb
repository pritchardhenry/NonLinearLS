{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Definition\n",
    "We consider a neural network that approximates a non-linear function $g(\\textbf{x})$ to be a function of the form:\n",
    "$$\\hat{y} = f_w (\\textbf{x}) = w_1 \\phi(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) + w_6 \\phi(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) + w_{11} \\phi (w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) + w_{16} \\quad (1)$$ \n",
    "Where the scalar output is $\\hat{y} \\in \\mathbb{R}$, the vector input is $\\textbf{x} \\in \\mathbb{R}^3$, and the weights that parametrize the network are $\\textbf{w} \\in \\mathbb{R}^{16}$ and $\\phi$: $\\mathbb{R} \\rightarrow \\mathbb{R}$  is defined as:\n",
    "$$\\phi(\\textbf{x}) = \\tanh(\\textbf{x}) = \\frac{e^\\textbf{x} - e^{-\\textbf{x}}}{e^\\textbf{x} + e^{-\\textbf{x}}} \\qquad \\text{(The hyperbolic tan function)}$$\n",
    "\n",
    "## Non-linear Least Squares Objective\n",
    "The goal is to determine the weights $w_1, ... w_{16}$ that best approximate a non-linear function $g(\\textbf{x})$ by minimizing the sum of squared errors, defined as:\n",
    "$$\\sum_{n=1}^N (f_w(\\textbf{x}^{(n)}) - y^{(n)})^2 = \\sum_{n=1}^N r_n(w)^2 \\qquad (2)$$\n",
    "Where $y^{(n)}$ is the output of $g(\\textbf{x})$ for the vector input $x^{(n)}$. \n",
    "\n",
    "## Tasks\n",
    "\n",
    "**1.** Using $(1)$, the gradient of $f_w(\\textbf{x})$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla_w f_w(\\textbf{x}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial w_1}f_w(\\textbf{x}) \\\\\n",
    "\\frac{\\partial}{\\partial w_2}f_w(\\textbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial w_{16}} f_w(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\phi(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_1 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_2 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_3 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) \\\\\n",
    "\\phi(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_1 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_2 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_3 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) \\\\\n",
    "\\phi(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_1 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_2 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_3 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $\\phi'(x) = \\frac{d}{dx} \\bigg( \\frac{e^x -e^{-x}}{e^x+e^{-x}} \\bigg) = \\frac{4e^{2x}}{(e^{2x}+1)^2}$\n",
    "\n",
    "**2.** The residual vector is defined as:\n",
    "$$\\textbf{r}(w) = \\big[ r_1 \\quad r_2\\quad ...\\quad r_N \\big] = \\big[ f_w(\\textbf{x}^{(1)}) - y^{(1)}\\quad f_w(\\textbf{x}^{(2)}) - y^{(2)} \\quad ... \\quad f_w(\\textbf{x}^{(N)}) - y^{(N)} \\big] $$\n",
    "The gradient of the residual vector is:\n",
    "$$\\nabla_w ||\\textbf{r}(w)||^2 = \\nabla_w \\bigg( \\sum_{n=1}^N r_n(w)^2 \\bigg) = 2 \\sum_{n=1}^N r_n(w) \\dot \\nabla_w r_n(w) \\quad \\text{(using the chain rule)} $$\n",
    "\\Which can be rewritten as:\n",
    "$$2(r_1(w)\\nabla_w r_1(w) + r_2(w)\\nabla_w r_2(w) + ... + r_N(w)\\nabla_w r_N(w)) = 2 \\big[\\nabla_w r_1(w) \\quad \\nabla_w r_2(w)  \\quad ... \\quad \\nabla_w r_N(w)\\big] \\begin{bmatrix}\n",
    "r_1(w)\\\\\n",
    "r_2(w)\\\\\n",
    "\\vdots\\\\\n",
    "r_N(w)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ = 2 \\begin{bmatrix}\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w r_1(w)\n",
    "\\end{bmatrix}^T \\begin{bmatrix}\n",
    "r_1(w)\\\\\n",
    "r_2(w)\\\\\n",
    "\\vdots\\\\\n",
    "r_N(w)\n",
    "\\end{bmatrix} = 2 \\textbf{Dr}(w)^T\\textbf{r}(w), \\quad \\text{where} \\: \\:\\textbf{Dr}(w) = \\begin{bmatrix}\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\nabla_w r_2(w)\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w r_N(w)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\nabla_w f_w(x^{(1)})\\\\\n",
    "\\nabla_w f_w(x^{(2)})\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w f_w(x^{(N)})\n",
    "\\end{bmatrix}$$\n",
    "The last step is due to the fact that $r_n(w)$ is defined as $y^{(n)} - f_w(x^{(n)})$ which means $\\nabla_w r_n(w) = \\nabla_w f_w(x^{(n)})$, since $y^{(n)}$ is not a function of $w$.\n",
    "  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e352a161a0982136"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71eaa4f8e56f70a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
