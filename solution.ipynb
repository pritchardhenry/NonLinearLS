{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Definition\n",
    "We consider a neural network that approximates a non-linear function $g(\\textbf{x})$ to be a function of the form:\n",
    "$$\\hat{y} = f_w (\\textbf{x}) = w_1 \\phi(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) + w_6 \\phi(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) + w_{11} \\phi (w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) + w_{16} \\quad$$ \n",
    "Where the scalar output is $\\hat{y} \\in \\mathbb{R}$, the vector input is $\\textbf{x} \\in \\mathbb{R}^3$, and the weights that parametrize the network are $\\textbf{w} \\in \\mathbb{R}^{16}$ and $\\phi$: $\\mathbb{R} \\rightarrow \\mathbb{R}$  is defined as:\n",
    "$$\\phi(\\textbf{x}) = \\tanh(\\textbf{x}) = \\frac{e^\\textbf{x} - e^{-\\textbf{x}}}{e^\\textbf{x} + e^{-\\textbf{x}}} \\qquad \\text{(The hyperbolic tan function)}$$\n",
    "\n",
    "## Non-linear Least Squares Objective\n",
    "The goal is to determine the weights $w_1, ... w_{16}$ that best approximate a non-linear function $g(\\textbf{x})$ by minimizing the sum of squared errors, defined as:\n",
    "$$\\sum_{n=1}^N (f_w(\\textbf{x}^{(n)}) - y^{(n)})^2 = \\sum_{n=1}^N r_n(w)^2$$\n",
    "Where $y^{(n)}$ is the output of $g(\\textbf{x})$ for the vector input $x^{(n)}$. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e352a161a0982136"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1)\n",
    " The gradient of $f_w(\\textbf{x})$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla_w f_w(\\textbf{x}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial w_1}f_w(\\textbf{x}) \\\\\n",
    "\\frac{\\partial}{\\partial w_2}f_w(\\textbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial w_{16}} f_w(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\phi(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_1 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_2 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_3 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) \\\\\n",
    "\\phi(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_1 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_2 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_3 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) \\\\\n",
    "\\phi(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_1 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_2 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_3 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $\\phi'(x) = \\frac{d}{dx} \\bigg( \\frac{e^x -e^{-x}}{e^x+e^{-x}} \\bigg) = \\frac{4e^{2x}}{(e^{2x}+1)^2}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8843c03745ca012"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2)\n",
    "$$\\textbf{r}(w) = \\big[ r_1 \\quad r_2\\quad ...\\quad r_N \\big] = \\big[ f_w(\\textbf{x}^{(1)}) - y^{(1)}\\quad f_w(\\textbf{x}^{(2)}) - y^{(2)} \\quad ... \\quad f_w(\\textbf{x}^{(N)}) - y^{(N)} \\big] $$\n",
    "The gradient of the sum of squared error is:\n",
    "$$\\nabla_w \\parallel \\textbf{r}(w) \\parallel^2 = \\nabla_w \\bigg( \\sum_{n=1}^N r_n(w)^2 \\bigg) = 2 \\sum_{n=1}^N r_n(w) \\dot \\nabla_w r_n(w) \\quad \\text{(using the chain rule)} $$\n",
    "Which can be rewritten as:\n",
    "$$2(r_1(w)\\nabla_w r_1(w) + r_2(w)\\nabla_w r_2(w) + ... + r_N(w)\\nabla_w r_N(w)) = 2 \\big[\\nabla_w r_1(w) \\quad \\nabla_w r_2(w)  \\quad ... \\quad \\nabla_w r_N(w)\\big] \\begin{bmatrix}\n",
    "r_1(w)\\\\\n",
    "r_2(w)\\\\\n",
    "\\vdots\\\\\n",
    "r_N(w)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ = 2 \\begin{bmatrix}\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w r_1(w)\n",
    "\\end{bmatrix}^T \\begin{bmatrix}\n",
    "r_1(w)\\\\\n",
    "r_2(w)\\\\\n",
    "\\vdots\\\\\n",
    "r_N(w)\n",
    "\\end{bmatrix} = 2 \\textbf{Dr}(w)^T\\textbf{r}(w), \\quad \\text{where} \\: \\:\\textbf{Dr}(w) = \\begin{bmatrix}\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\nabla_w r_2(w)\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w r_N(w)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\nabla_w f_w(x^{(1)})\\\\\n",
    "\\nabla_w f_w(x^{(2)})\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w f_w(x^{(N)})\n",
    "\\end{bmatrix}$$\n",
    "The last step is due to the fact that $r_n(w)$ is defined as $y^{(n)} - f_w(x^{(n)})$ which means $\\nabla_w r_n(w) = \\nabla_w f_w(x^{(n)})$, since $y^{(n)}$ is not a function of $w$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc31e3ea2a62775"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3)\n",
    "\n",
    "The training data for the neural network to approximate the non-linear function $g(\\textbf{x}) = x_1 x_2 + x_3$ will consist of $N=500$ randomly generated points, $\\mathbf{x}^{(n)} = \\begin{bmatrix} x_1^{(n)} \\\\ x_2^{(n)} \\\\ x_3^{(n)} \\end{bmatrix} \\in \\mathbb{R}^3$, such that max$ \\{ |x_1^{(n)}|,|x_2^{(n)}|,|x_3^{(n)}| \\} \\leq \\Gamma = 1$ for all $n = 1,2,...,N$.\n",
    "\n",
    "The training pairs $\\big(\\textbf{x}^{(n)}, y^{(n)} \\big) _{n=1}^N$ will be used to minimize the training loss with respect to $\\textbf{w}$, $l(\\textbf{w}) = \\sum_{n=1}^N r_n^2(\\textbf{w}) + \\lambda \\parallel \\textbf{w} \\parallel^2_2$\n",
    "The training data $\\textbf{x}$ and $y$ is generated below. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8748e7aee523327d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from numpy import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T07:11:18.285357300Z",
     "start_time": "2023-12-03T07:11:18.277348900Z"
    }
   },
   "id": "f71eaa4f8e56f70a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Generates num training points x_1, x_2, x_3 in R^3 between -gamma and +gamma and y = function(x_1, x_2, x_3)\n",
    "def random_data(num = 500, gamma = 1, function = lambda x_1, x_2, x_3: x_1*x_2 + x_3):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(num):\n",
    "        x_1 = random.rand.uniform(-gamma, gamma)\n",
    "        x_2 = random.rand.uniform(-gamma, gamma)\n",
    "        x_3 = random.rand.uniform(-gamma, gamma)\n",
    "        x_train.append([x_1, x_2, x_3])\n",
    "        y_train.append(function(x_1, x_2, x_3))\n",
    "    return x_train, y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T07:11:05.982259800Z",
     "start_time": "2023-12-03T07:11:05.969541400Z"
    }
   },
   "id": "e9cb0dee58c6eeee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**a)** The Levenberg-Marquardt algorithm is a variation of the Gauss-Newton algorithm and addresses the issue of the algorithm diverging when the estimated points get further and further away. The algorithm to solve for $w$, the correct weights to approximate $g(\\textbf{x})$ is as follows:\n",
    "1. The *affine approximation* at the current iterate is calculated by first order Taylor Approximation given by:\n",
    "$$\\r(w;w^{(k)}) = r(w^{(k)}) + \\textbf{D}r(w^{(k)})(w-w^{(k)})$$\n",
    "2.  Next, the *tentative iterate*, $w^{(k+1)}$ minimizes $\\parallel r(w;w^{(k)})\\parallel^2 + \\lambda^{(k)}\\parallel w-w^{(k)}\\parallel ^2 $. $w^{(k+1)}$ is given by:\n",
    "$$w^{(k+1)} = w^{(k)} - \\big( \\textbf{D}r(w^{(k)})^T \\textbf{D}r(w^{(k)}) + \\lambda ^{(k)} I \\big)^{-1}\\textbf{D}r(w^{(k)})^T r(w^{(k)})]$$\n",
    "3. Finally, the *tentative iterate* is checked against the previous one. If $\\parallel r(w^{(k+1)}) \\parallel ^2 < \\parallel r(w^{(k)}) \\parallel^2$, $w^{(k+1)}$ is accepted and $lambda^{(k+1)} = .8\\lambda^{(k)}$. Otherwise, $w^{(k+1)} = w^{(k)}$ and $\\lambda^{(k+1)} = 2\\lambda^{(k)}$.\n",
    "\n",
    "The algorithm terminates when either terms $\\parallel r(w^{(k+1)} \\parallel ^2$ or $\\parallel 2\\textbf{D}r(w^{(k+1)})^T r(w^{(k+1)}) \\parallel$ are small enough.\n",
    "\n",
    "The implementation is below:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60223ddd187f112d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# derivative of tanh(x)\n",
    "def tanh_p(x):\n",
    "    e = np.exp(1)\n",
    "    return (4*e**(2*x))/((e**(2*x)+1)**2)\n",
    "\n",
    "def f_w(x, weights):\n",
    "    \n",
    "\n",
    "# (training) loss with respect to w\n",
    "def l(w):\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T08:56:19.740826Z",
     "start_time": "2023-12-03T08:56:19.729441700Z"
    }
   },
   "id": "af009e1171921a16"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
