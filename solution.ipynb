{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Definition\n",
    "We consider a neural network that approximates a non-linear function $g(\\textbf{x})$ to be a function of the form:\n",
    "$$\\hat{y} = f_w (\\textbf{x}) = w_1 \\phi(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) + w_6 \\phi(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) + w_{11} \\phi (w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) + w_{16} \\quad$$ \n",
    "Where the scalar output is $\\hat{y} \\in \\mathbb{R}$, the vector input is $\\textbf{x} \\in \\mathbb{R}^3$, and the weights that parametrize the network are $\\textbf{w} \\in \\mathbb{R}^{16}$ and $\\phi$: $\\mathbb{R} \\rightarrow \\mathbb{R}$  is defined as:\n",
    "$$\\phi(\\textbf{x}) = \\tanh(\\textbf{x}) = \\frac{e^\\textbf{x} - e^{-\\textbf{x}}}{e^\\textbf{x} + e^{-\\textbf{x}}} \\qquad \\text{(The hyperbolic tan function)}$$\n",
    "\n",
    "## Non-linear Least Squares Objective\n",
    "The goal is to determine the weights $w_1, ... w_{16}$ that best approximate a non-linear function $g(\\textbf{x})$ by minimizing the sum of squared errors, defined as:\n",
    "$$\\sum_{n=1}^N (f_w(\\textbf{x}^{(n)}) - y^{(n)})^2 = \\sum_{n=1}^N r_n(w)^2$$\n",
    "Where $y^{(n)}$ is the output of $g(\\textbf{x})$ for the vector input $x^{(n)}$. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e352a161a0982136"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1)\n",
    " The gradient of $f_w(\\textbf{x})$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla_w f_w(\\textbf{x}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial w_1}f_w(\\textbf{x}) \\\\\n",
    "\\frac{\\partial}{\\partial w_2}f_w(\\textbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial w_{16}} f_w(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\phi(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_1 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_2 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) x_3 \\\\\n",
    "w_1 \\phi'(w_2 x_1 + w_3 x_2 + w_4 x_3 + w_5) \\\\\n",
    "\\phi(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_1 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_2 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) x_3 \\\\\n",
    "w_6 \\phi'(w_7 x_1 + w_8 x_2 + w_9 x_3 + w_{10}) \\\\\n",
    "\\phi(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_1 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_2 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) x_3 \\\\\n",
    "w_{11} \\phi'(w_{12}x_1 + w_{13}x_2 + w_{14}x_3 + w_{15}) \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $\\phi'(x) = \\frac{d}{dx} \\bigg( \\frac{e^x -e^{-x}}{e^x+e^{-x}} \\bigg) = \\frac{4e^{2x}}{(e^{2x}+1)^2}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8843c03745ca012"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2)\n",
    "$$\\textbf{r}(w) = \\big[ r_1 \\quad r_2\\quad ...\\quad r_N \\big] = \\big[ f_w(\\textbf{x}^{(1)}) - y^{(1)}\\quad f_w(\\textbf{x}^{(2)}) - y^{(2)} \\quad ... \\quad f_w(\\textbf{x}^{(N)}) - y^{(N)} \\big] $$\n",
    "The gradient of the sum of squared error is:\n",
    "$$\\nabla_w \\parallel \\textbf{r}(w) \\parallel^2 = \\nabla_w \\bigg( \\sum_{n=1}^N r_n(w)^2 \\bigg) = 2 \\sum_{n=1}^N r_n(w) \\dot \\nabla_w r_n(w) \\quad \\text{(using the chain rule)} $$\n",
    "Which can be rewritten as:\n",
    "$$2(r_1(w)\\nabla_w r_1(w) + r_2(w)\\nabla_w r_2(w) + ... + r_N(w)\\nabla_w r_N(w)) = 2 \\big[\\nabla_w r_1(w) \\quad \\nabla_w r_2(w)  \\quad ... \\quad \\nabla_w r_N(w)\\big] \\begin{bmatrix}\n",
    "r_1(w)\\\\\n",
    "r_2(w)\\\\\n",
    "\\vdots\\\\\n",
    "r_N(w)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ = 2 \\begin{bmatrix}\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w r_1(w)\n",
    "\\end{bmatrix}^T \\begin{bmatrix}\n",
    "r_1(w)\\\\\n",
    "r_2(w)\\\\\n",
    "\\vdots\\\\\n",
    "r_N(w)\n",
    "\\end{bmatrix} = 2 \\textbf{Dr}(w)^T\\textbf{r}(w), \\quad \\text{where} \\: \\:\\textbf{Dr}(w) = \\begin{bmatrix}\n",
    "\\nabla_w r_1(w)\\\\\n",
    "\\nabla_w r_2(w)\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w r_N(w)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\nabla_w f_w(x^{(1)})\\\\\n",
    "\\nabla_w f_w(x^{(2)})\\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_w f_w(x^{(N)})\n",
    "\\end{bmatrix}$$\n",
    "The last step is due to the fact that $r_n(w)$ is defined as $y^{(n)} - f_w(x^{(n)})$ which means $\\nabla_w r_n(w) = \\nabla_w f_w(x^{(n)})$, since $y^{(n)}$ is not a function of $w$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc31e3ea2a62775"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3)\n",
    "\n",
    "The training data for the neural network to approximate the non-linear function $g(\\textbf{x}) = x_1 x_2 + x_3$ will consist of $N=500$ randomly generated points, $\\mathbf{x}^{(n)} = \\begin{bmatrix} x_1^{(n)} \\\\ x_2^{(n)} \\\\ x_3^{(n)} \\end{bmatrix} \\in \\mathbb{R}^3$, such that max$ \\{ |x_1^{(n)}|,|x_2^{(n)}|,|x_3^{(n)}| \\} \\leq \\Gamma = 1$ for all $n = 1,2,...,N$.\n",
    "\n",
    "The training pairs $\\big(\\textbf{x}^{(n)}, y^{(n)} \\big) _{n=1}^N$ will be used to minimize the training loss with respect to $\\textbf{w}$, $l(\\textbf{w}) = \\sum_{n=1}^N r_n^2(\\textbf{w}) + \\lambda \\parallel \\textbf{w} \\parallel^2_2$\n",
    "The training data $\\textbf{x}$ and $y$ is generated below. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8748e7aee523327d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from numpy import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T07:11:18.285357300Z",
     "start_time": "2023-12-03T07:11:18.277348900Z"
    }
   },
   "id": "f71eaa4f8e56f70a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Generates num training points x_1, x_2, x_3 in R^3 between -gamma and +gamma and y = function(x_1, x_2, x_3)\n",
    "def random_data(num = 500, gamma = 1, function = lambda x_1, x_2, x_3: x_1*x_2 + x_3):\n",
    "    x_train = np.empty((0,))\n",
    "    y_train = np.empty((0,))\n",
    "    for i in range(num):\n",
    "        x_1 = random.rand.uniform(-gamma, gamma)\n",
    "        x_2 = random.rand.uniform(-gamma, gamma)\n",
    "        x_3 = random.rand.uniform(-gamma, gamma)\n",
    "        np.append(x_train, [x_1, x_2, x_3])\n",
    "        np.append(y_train, function(x_1, x_2, x_3))\n",
    "    return x_train, y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T23:55:49.488155800Z",
     "start_time": "2023-12-03T23:55:49.485960600Z"
    }
   },
   "id": "e9cb0dee58c6eeee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**a)** The Levenberg-Marquardt algorithm is a variation of the Gauss-Newton algorithm and addresses the issue of the algorithm diverging when the estimated points get further and further away. The algorithm to solve for $w$, the correct weights to approximate $g(\\textbf{x})$ is as follows:\n",
    "1. The *affine approximation* at the current iterate is calculated by first order Taylor Approximation given by:\n",
    "$$r(w;w^{(k)}) = r(w^{(k)}) + \\textbf{D}r(w^{(k)})(w-w^{(k)})$$\n",
    "2.  Next, the *tentative iterate*, $w^{(k+1)}$ minimizes $\\parallel r(w;w^{(k)})\\parallel^2 + \\lambda^{(k)}\\parallel w-w^{(k)}\\parallel ^2 $. $w^{(k+1)}$ is given by:\n",
    "$$w^{(k+1)} = w^{(k)} - \\big( \\textbf{D}r(w^{(k)})^T \\textbf{D}r(w^{(k)}) + \\lambda ^{(k)} I \\big)^{-1}\\textbf{D}r(w^{(k)})^T r(w^{(k)})]$$\n",
    "3. Finally, the *tentative iterate* is checked against the previous one. If $\\parallel r(w^{(k+1)}) \\parallel ^2 < \\parallel r(w^{(k)}) \\parallel^2$, $w^{(k+1)}$ is accepted and $\\lambda^{(k+1)} = .8\\lambda^{(k)}$. Otherwise, $w^{(k+1)} = w^{(k)}$ and $\\lambda^{(k+1)} = 2\\lambda^{(k)}$.\n",
    "\n",
    "The algorithm terminates when either terms $\\parallel r(w^{(k+1)} \\parallel ^2$ or $\\parallel 2\\textbf{D}r(w^{(k+1)})^T r(w^{(k+1)}) \\parallel$ are small enough.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60223ddd187f112d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e5ae5d30bf3043"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "859c536519731993"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T23:29:29.509470200Z",
     "start_time": "2023-12-03T23:29:29.413267400Z"
    }
   },
   "id": "ab3eae614e4d8f24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f_w(x), the estimate of y given xn=[x_1, x_2, x_3] and w = [w_1 ... w_16]\n",
    "def f_w(x, w):\n",
    "    x_1, x_2, x_3 = x\n",
    "    w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9, w_10, w_11, w_12, w_13, w_14, w_15, w_16 = w\n",
    "    return w_1*np.tanh(w_2*x_1 + w_3*x_2 + w_4*x_3 + w_5) + w_6*np.tanh(w_7*x_1 + w_8*x_2 + w_9*x_3 + w_10) + w_11*np.tanh(w_12*x_1 + w_13*x_2 + w_14*x_3 + w_15) + w_16\n",
    "\n",
    "# vector of f_w(x) for all x in training data\n",
    "def f_w_vec(x, w):\n",
    "    y = np.empty((0,))\n",
    "    for x_n in x:\n",
    "        y = np.append(y, f_w(x_n, w))\n",
    "    return y\n",
    "\n",
    "# derivative of tanh(x)\n",
    "def tanh_p(x):\n",
    "    e = np.exp(1)\n",
    "    return (4 * e ** (2 * x)) / ((e ** (2 * x) + 1) ** 2)\n",
    "\n",
    "# gradient of f_w(x) with respect to w\n",
    "def nabla_f_w(x, w):\n",
    "    x_1, x_2, x_3 = x\n",
    "    w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9, w_10, w_11, w_12, w_13, w_14, w_15, w_16 = w\n",
    "    return np.array([np.tanh(w_2*x_1 + w_3*x_2 + w_4*x_3 + w_5), w_1*tanh_p(w_2*x_1 + w_3*x_2 + w_4*x_3 + w_5)*x_1, w_1*tanh_p(w_2*x_1 + w_3*x_2 + w_4*x_3 + w_5)*x_2, w_1*tanh_p(w_2*x_1 + w_3*x_2 + w_4*x_3 + w_5)*x_3, w_1*tanh_p(w_2*x_1 + w_3*x_2 + w_4*x_3 + w_5), np.tanh(w_7*x_1 + w_8*x_2 + w_9*x_3 + w_10), w_6*tanh_p(w_7*x_1 + w_8*x_2 + w_9*x_3 + w_10)*x_1, w_6*tanh_p(w_7*x_1 + w_8*x_2 + w_9*x_3 + w_10)*x_2, w_6*tanh_p(w_7*x_1 + w_8*x_2 + w_9*x_3 + w_10)*x_3, w_6*tanh_p(w_7*x_1 + w_8*x_2 + w_9*x_3 + w_10), np.tanh(w_12*x_1 + w_13*x_2 + w_14*x_3 + w_15), w_11*tanh_p(w_12*x_1 + w_13*x_2 + w_14*x_3 + w_15)*x_1, w_11*tanh_p(w_12*x_1 + w_13*x_2 + w_14*x_3 + w_15)*x_2, w_11*tanh_p(w_12*x_1 + w_13*x_2 + w_14*x_3 + w_15)*x_3, w_11*tanh_p(w_12*x_1 + w_13*x_2 + w_14*x_3 + w_15), 1])\n",
    "\n",
    "# gradient of r_n(w) with respect to w (same as nabla_f_w(x, w), implemented for clarity)\n",
    "def nabla_r_n(x, w):\n",
    "    return nabla_f_w(x, w) \n",
    "\n",
    "# derivative matrix of r_n(w) for all x in training data, made up of stacked gradients\n",
    "def dr_w_matrix(x, w):\n",
    "    dr_w = np.empty((0,16))\n",
    "    for x_n in x:\n",
    "        mat = np.vstack((dr_w, nabla_r_n(x_n, w)))\n",
    "    return dr_w\n",
    "\n",
    "# find the next iterate of w given the current iterate w_k, the training data x, y, and l (lambda)\n",
    "def next_iterate(last_iterate, x, y, l):\n",
    "    w_k = last_iterate\n",
    "    dr = dr_w_matrix(x, w_k)\n",
    "    r = f_w_vec(x, w_k) - y\n",
    "    w_k_1 = w_k - np.linalg.inv(dr.T @ dr + l*np.identity(16)) @ dr.T @ r\n",
    "    return w_k_1\n",
    "\n",
    "# training loss with respect to weights w, given training data (x,y) and l (lambda) for the iterate\n",
    "def training_loss(w, x, y, l):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += (f_w(x[i], w) - y[i])**2 + l*np.linalg.norm(w)**2\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62c4ac88ad7a9239"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "725fc53b671e499f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def make_weights():\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T00:47:11.830081800Z",
     "start_time": "2023-12-04T00:47:11.813527Z"
    }
   },
   "id": "af009e1171921a16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "178c5bc664d6189"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
